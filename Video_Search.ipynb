{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Scene Extraction Based on Query\n",
    "This project provides a solution for extracting the most relevant scene from a video based on a text query. By leveraging the power of OpenAI's CLIP model, it compares the content of each scene in the video to the query and extracts the scene with the highest similarity.\n",
    "\n",
    "### Features\n",
    "Scene Detection: Automatically detects scenes in a video using the scenedetect library.\n",
    "\n",
    "Frame Extraction: Extracts frames from detected scenes for further processing.\n",
    "\n",
    "Text-Image Similarity: Uses CLIP to compute the similarity between text queries and video frames.\n",
    "\n",
    "Scene Extraction: Extracts and saves the most relevant scene based on the query as a new video file.\n",
    "\n",
    "Gradio Interface: Provides a user-friendly web interface to easily upload videos and input search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import scenedetect\n",
    "import logging\n",
    "import gradio as gr\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Setting up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger()\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "\n",
    "    def detect_scenes(self, video_path):\n",
    "        \"\"\"\n",
    "        Detect scenes in the video using SceneDetect.\n",
    "        \"\"\"\n",
    "        scene_manager = scenedetect.SceneManager()\n",
    "        scene_manager.add_detector(scenedetect.detectors.ContentDetector())\n",
    "        video = scenedetect.open_video(video_path)\n",
    "        scene_manager.detect_scenes(video)\n",
    "        return scene_manager.get_scene_list()\n",
    "\n",
    "    def extract_frames(self, video_path, scene_start, scene_end):\n",
    "        \"\"\"\n",
    "        Extract frames from a specific scene in the video.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, scene_start)\n",
    "        frames = []\n",
    "        for t in range(scene_start, scene_end + 1):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, t)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                frames.append(pil_image)\n",
    "\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def get_query_embedding(self, query):\n",
    "        \"\"\"\n",
    "        Get the embedding of the query using CLIP.\n",
    "        \"\"\"\n",
    "        text_inputs = self.processor(text=[query], return_tensors=\"pt\", padding=True).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model.get_text_features(input_ids=text_inputs.input_ids)\n",
    "        return query_embedding\n",
    "\n",
    "    def compute_frame_embeddings(self, frames):\n",
    "        \"\"\"\n",
    "        Compute the embeddings of frames using CLIP.\n",
    "        \"\"\"\n",
    "        frame_embeddings = []\n",
    "        for frame in frames:\n",
    "            frame_tensor = self.processor(images=frame, return_tensors=\"pt\").pixel_values.to(self.device)\n",
    "            with torch.no_grad():\n",
    "                frame_embedding = self.model.get_image_features(pixel_values=frame_tensor)\n",
    "                frame_embeddings.append(frame_embedding)\n",
    "        \n",
    "        return torch.stack(frame_embeddings)\n",
    "\n",
    "    def process_scene(self, frames_in_scene, query_embedding):\n",
    "        \"\"\"\n",
    "        Process each scene to compute the similarity between the frames and the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scene_embeddings = self.compute_frame_embeddings(frames_in_scene)\n",
    "            similarities = torch.nn.functional.cosine_similarity(query_embedding, scene_embeddings)\n",
    "            return similarities.mean().item()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing scene: {e}\")\n",
    "            return -1\n",
    "\n",
    "    def extract_scene_from_video(self, video_path, scene_start, scene_end, output_video_path):\n",
    "        \"\"\"\n",
    "        Extract the most relevant scene from the video and save to a fixed file.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        output_video = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps,\n",
    "                                       (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, scene_start)\n",
    "        frames = []\n",
    "        for t in range(scene_start, scene_end + 1):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, t)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frames.append(frame)\n",
    "\n",
    "        for frame in frames:\n",
    "            output_video.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        output_video.release()\n",
    "\n",
    "    def process_video(self, video_path, query, output_scene_path=\"output_scene.mp4\"):\n",
    "        \"\"\"\n",
    "        Main function to process the video, detect scenes, compute embeddings, and extract the relevant scene.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.info(f\"Step 1: Detect scenes\")\n",
    "        scene_list = self.detect_scenes(video_path)\n",
    "\n",
    "        logger.info(f\"Detected {len(scene_list)} scenes\")\n",
    "\n",
    "        logger.info(f\"Step 2: Extract frames for each scene\")\n",
    "        frames = []\n",
    "        for scene in scene_list:\n",
    "            scene_start, scene_end = map(int, scene)\n",
    "            frames_in_scene = self.extract_frames(video_path, scene_start, scene_end)\n",
    "            frames.append((frames_in_scene, scene_start, scene_end))\n",
    "\n",
    "        logger.info(f\"Extracted {len(frames)} frames from {len(frames)} scenes\")\n",
    "\n",
    "        logger.info(f\"Step 3: Get query embedding\")\n",
    "        query_embedding = self.get_query_embedding(query)\n",
    "\n",
    "        logger.info(f\"Step 4: Process each scene sequentially to find the most relevant one\")\n",
    "        top_scene = None\n",
    "        highest_similarity = -1\n",
    "\n",
    "        for frames_in_scene, scene_start, scene_end in frames:\n",
    "            scene_similarity = self.process_scene(frames_in_scene, query_embedding)\n",
    "            if scene_similarity > highest_similarity:\n",
    "                highest_similarity = scene_similarity\n",
    "                top_scene = (scene_start, scene_end)\n",
    "\n",
    "        logger.info(f\"Step 5: Extract the most relevant scene\")\n",
    "        if top_scene:\n",
    "            scene_start, scene_end = top_scene\n",
    "            self.extract_scene_from_video(video_path, scene_start, scene_end, output_scene_path)\n",
    "            logger.info(f\"Extracted scene saved as {output_scene_path}\")\n",
    "            return output_scene_path\n",
    "        else:\n",
    "            logger.info(\"No relevant scene found.\")\n",
    "            return None\n",
    "\n",
    "# Gradio interface\n",
    "def process_video_with_gradio(video_file, query):\n",
    "    try:\n",
    "        error_message = None\n",
    "\n",
    "        # Check if query is empty\n",
    "        if not query or not video_file:\n",
    "            error_message = \"Query and Video are required. Please enter a search term.\"\n",
    "\n",
    "        if error_message:\n",
    "            return [None, error_message]\n",
    "    \n",
    "        logger.info(\"initialize Video Processor\")\n",
    "        video_processor = VideoProcessor()\n",
    "\n",
    "        # Process the video and get the output scene\n",
    "        output_scene_path = video_processor.process_video(video_file, query)\n",
    "\n",
    "        if output_scene_path:\n",
    "            return [output_scene_path, None]\n",
    "        else:\n",
    "            return [None, \"No relevant scene found.\"]\n",
    "    except Exception as ex:\n",
    "        logger.info(f\"Process video exception: {ex}\")\n",
    "\n",
    "\n",
    "\n",
    "# Set up Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_video_with_gradio,\n",
    "    inputs=[\n",
    "        gr.Video(),\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Enter the search query\")\n",
    "    ],\n",
    "    outputs=[gr.Video(label=\"Extracted Video\"), gr.Label(label=\"Error Message\", value=\"\", elem_id=\"error-message\")],\n",
    "    live=False,\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "iface.css = \"\"\"\n",
    "    #error-message {\n",
    "        color: red;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
