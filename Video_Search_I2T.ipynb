{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video Scene Extraction with Image Captioning and Text Query\n",
    "\n",
    "This project extends the functionality of video scene extraction by incorporating an image captioning model (BLIP) alongside the CLIP model for improved query-to-video relevance. It enables users to extract the most relevant scene from a video based on a text query, using both visual context and automatically generated captions for better scene understanding.\n",
    "\n",
    "### Features\n",
    "Scene Detection: Automatically detects scenes in a video using the scenedetect library.\n",
    "\n",
    "Frame Extraction: Extracts frames from each detected scene for analysis.\n",
    "\n",
    "Image-to-Text (I2T) Embedding: Uses the BLIP model to generate captions for each frame, improving the contextual understanding of video content.\n",
    "\n",
    "Text-Image Similarity: Uses CLIP to compute the similarity between the provided text query and the frames' captions.\n",
    "\n",
    "Scene Extraction: Extracts and saves the most relevant scene based on the highest similarity to the query as a new video.\n",
    "\n",
    "Gradio Interface: A simple web interface to upload videos and input search queries for scene extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger('my_custom_logger')\n",
    "\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "ch = logging.StreamHandler()\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "ch.setFormatter(formatter)\n",
    "\n",
    "logger.addHandler(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel, BlipProcessor, BlipForConditionalGeneration\n",
    "from PIL import Image\n",
    "import scenedetect\n",
    "import logging\n",
    "import gradio as gr\n",
    "import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class VideoProcessor:\n",
    "    def __init__(self, device=\"cuda\"):\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "\n",
    "        self.blip_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n",
    "        self.blip_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\").to(self.device)\n",
    "\n",
    "    def detect_scenes(self, video_path):\n",
    "        \"\"\"\n",
    "        Detect scenes in the video using SceneDetect.\n",
    "        \"\"\"\n",
    "        scene_manager = scenedetect.SceneManager()\n",
    "        scene_manager.add_detector(scenedetect.detectors.ContentDetector())\n",
    "        video = scenedetect.open_video(video_path)\n",
    "        scene_manager.detect_scenes(video)\n",
    "        return scene_manager.get_scene_list()\n",
    "\n",
    "    def extract_frames(self, video_path, scene_start, scene_end):\n",
    "        \"\"\"\n",
    "        Extract frames from a specific scene in the video.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, scene_start)\n",
    "        frames = []\n",
    "        for t in range(scene_start, scene_end + 1):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, t)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                frames.append(pil_image)\n",
    "\n",
    "        cap.release()\n",
    "        return frames\n",
    "\n",
    "    def get_query_embedding(self, query):\n",
    "        \"\"\"\n",
    "        Get the embedding of the query using CLIP.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            text_inputs = self.processor(text=[query], return_tensors=\"pt\", padding=True).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                query_embedding = self.model.get_text_features(input_ids=text_inputs.input_ids)\n",
    "            return query_embedding\n",
    "        except Exception as ex:\n",
    "            logger.error('CLIP query embeddings exception: {ex}')\n",
    "            return\n",
    "\n",
    "    def compute_I2T_embeddings(self, frames):\n",
    "        \"\"\"\n",
    "        Compute the embeddings of frames query using BLIP.\n",
    "        \"\"\"\n",
    "        query_embeddings = []\n",
    "        count = 0\n",
    "        for frame in frames:\n",
    "            try:\n",
    "                inputs = self.blip_processor(frame, return_tensors=\"pt\").to(self.device)\n",
    "                out = self.blip_model.generate(**inputs)\n",
    "                query = self.blip_processor.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "                count+=1\n",
    "                logger.debug(f'frame NO. {count} - query: {query}')\n",
    "\n",
    "                query_embedding = self.get_query_embedding(query)\n",
    "                query_embeddings.append(query_embedding)\n",
    "                logger.info(f'count: {count} end')\n",
    "            except Exception as ex:\n",
    "                logger.error(f'I2T embeddings exception: {ex}')\n",
    "                continue\n",
    "        \n",
    "        return torch.stack(query_embeddings)\n",
    "\n",
    "    def process_scene(self, frames_in_scene, query_embedding):\n",
    "        \"\"\"\n",
    "        Process each scene to compute the similarity between the frames and the query.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            scene_embeddings = self.compute_I2T_embeddings(frames_in_scene)\n",
    "            similarities = torch.nn.functional.cosine_similarity(query_embedding, scene_embeddings)\n",
    "            return similarities.mean().item()\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing scene: {e}\")\n",
    "            return -1\n",
    "\n",
    "    def extract_scene_from_video(self, video_path, scene_start, scene_end, output_video_path):\n",
    "        \"\"\"\n",
    "        Extract the most relevant scene from the video and save to a fixed file.\n",
    "        \"\"\"\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        output_video = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps,\n",
    "                                       (int(cap.get(3)), int(cap.get(4))))\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, scene_start)\n",
    "        frames = []\n",
    "        for t in range(scene_start, scene_end + 1):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, t)\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                frames.append(frame)\n",
    "\n",
    "        for frame in frames:\n",
    "            output_video.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        output_video.release()\n",
    "\n",
    "    def process_video(self, video_path, query, output_scene_path=\"output_scene.mp4\"):\n",
    "        \"\"\"\n",
    "        Main function to process the video, detect scenes, compute embeddings, and extract the relevant scene.\n",
    "        \"\"\"\n",
    "\n",
    "        logger.debug(f\"Step 1: Detect scenes\")\n",
    "        scene_list = self.detect_scenes(video_path)\n",
    "\n",
    "        logger.info(f\"Detected {len(scene_list)} scenes\")\n",
    "\n",
    "        logger.debug(f\"Step 2: Extract frames for each scene\")\n",
    "        frames = []\n",
    "        for scene in scene_list:\n",
    "            scene_start, scene_end = map(int, scene)\n",
    "            frames_in_scene = self.extract_frames(video_path, scene_start, scene_end)\n",
    "            frames.append((frames_in_scene, scene_start, scene_end))\n",
    "\n",
    "        logger.info(f\"Extracted {len(frames)} frames from {len(frames)} scenes\")\n",
    "\n",
    "        logger.debug(f\"Step 3: Get query embedding\")\n",
    "        query_embedding = self.get_query_embedding(query)\n",
    "\n",
    "        logger.debug(f\"Step 4: Process each scene sequentially to find the most relevant one\")\n",
    "        top_scene = None\n",
    "        highest_similarity = -1\n",
    "\n",
    "        for frames_in_scene, scene_start, scene_end in frames:\n",
    "            scene_similarity = self.process_scene(frames_in_scene, query_embedding)\n",
    "            if scene_similarity > highest_similarity:\n",
    "                highest_similarity = scene_similarity\n",
    "                top_scene = (scene_start, scene_end)\n",
    "\n",
    "        logger.debug(f\"Step 5: Extract the most relevant scene\")\n",
    "        if top_scene:\n",
    "            scene_start, scene_end = top_scene\n",
    "            self.extract_scene_from_video(video_path, scene_start, scene_end, output_scene_path)\n",
    "            logger.info(f\"Extracted scene saved as {output_scene_path}\")\n",
    "            return output_scene_path\n",
    "        else:\n",
    "            logger.info(\"No relevant scene found.\")\n",
    "            return None\n",
    "\n",
    "# Gradio interface\n",
    "def process_video_with_gradio(video_file, query):\n",
    "    try:\n",
    "        error_message = None\n",
    "\n",
    "        # Check if query is empty\n",
    "        if not query or not video_file:\n",
    "            error_message = \"Query and Video are required. Please enter a search term.\"\n",
    "\n",
    "        if error_message:\n",
    "            return [None, error_message]\n",
    "    \n",
    "        logger.info(\"initialize Video Processor\")\n",
    "        video_processor = VideoProcessor()\n",
    "\n",
    "        # Process the video and get the output scene\n",
    "        output_scene_path = video_processor.process_video(video_file, query, f\"output_{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M')}\")\n",
    "\n",
    "        if output_scene_path:\n",
    "            return [output_scene_path, None]\n",
    "        else:\n",
    "            return [None, \"No relevant scene found.\"]\n",
    "    except Exception as ex:\n",
    "        logger.error(f\"Process video exception: {ex}\")\n",
    "\n",
    "\n",
    "# Set up Gradio interface\n",
    "iface = gr.Interface(\n",
    "    fn=process_video_with_gradio,\n",
    "    inputs=[\n",
    "        gr.Video(),\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Enter the search query\")\n",
    "    ],\n",
    "    outputs=[gr.Video(label=\"Extracted Video\"), gr.Label(label=\"Error Message\", value=\"\", elem_id=\"error-message\")],\n",
    "    live=False,\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "iface.css = \"\"\"\n",
    "    #error-message {\n",
    "        color: red;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    iface.launch()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
